{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN2eQDodeJjGy4jXatKguPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/huggingface/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "# !pip list"
      ],
      "metadata": {
        "id": "O0ilwt5TG3cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqJcuJgjEutS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparmaeters\n",
        "NUM_CLASSES = 6\n",
        "BATCH_SIZE = 16 #128 # 16\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5"
      ],
      "metadata": {
        "id": "ZkSe_9wPE94G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'"
      ],
      "metadata": {
        "id": "RchwwnUVGcTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from one-hot encoded CSV file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "csv_file = '/content/drive/MyDrive/datasets/multi-label/train.csv'\n",
        "data = pd.read_csv(csv_file, encoding='utf-8')"
      ],
      "metadata": {
        "id": "g4P0VFsrGols"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "id": "pBJUuPRUMliw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빠른 실험을 위해 데이터셋의 일부만 사용한다. 10%\n",
        "data, _ = torch.utils.data.random_split(data, [0.1, 0.9])"
      ],
      "metadata": {
        "id": "GWRQ93zwLhan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))\n",
        "\n",
        "print(data.columns())"
      ],
      "metadata": {
        "id": "VJznF_NMMfjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the data and labels\n",
        "texts = data['ABSTRACT'].tolist()\n",
        "labels = data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values"
      ],
      "metadata": {
        "id": "JCtphSLbHP6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 글자 자르는 부분인데 일단은 주석처리 해보자.\n",
        "# max_sequence_length = 512\n",
        "# texts = [text[:max_sequence_length] for text in texts]"
      ],
      "metadata": {
        "id": "iiZL7eIAHnRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode the texts\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, num_labels=NUM_CLASSES)\n",
        "encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "RlXTwESZHurX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to Pytorch tensors\n",
        "label_tensor = torch.tensor(labels, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Gg0RD6tjIHr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encoded_texts['input_ids']\n",
        "attention_mask = encoded_texts['attention_mask']"
      ],
      "metadata": {
        "id": "7vSCX7GqIQLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, val_input_ids, \\\n",
        "train_attention_mask, val_attention_mask, \\\n",
        "train_labels, val_labels = train_test_split(input_ids, attention_mask, label_tensor, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "09T4_I68IaI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids"
      ],
      "metadata": {
        "id": "g36JEumhK4YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_attention_mask[:3]"
      ],
      "metadata": {
        "id": "S-uGrUMHK6ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_data = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Dataloader class는 batch기반의 딥러닝모델 학습을 위해서 mini batch를 만들어주는 역할을 한다.\n",
        "# dataloader를 통해 dataset의 전체 데이터가 batch size로 slice된다.\n",
        "# 앞서 만들었던 dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 알아서 병렬처리)을 통해 batch를 만들어준다.\n",
        "# 서버에서 돌릴 때는 cpu num_worker를 조절해서 load속도를 올릴 수 있지만, PC에서는 default로 설정해야 오류가 안난다."
      ],
      "metadata": {
        "id": "4gW2tdx1IrLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "# Lists to store loss and accuracy values\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1,2,1)\n",
        "loss_plot, = plt.plot([], [], label='Training Loss', color='blue')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "accuracy_plot, = plt.plot([], [], label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "l7ncCa-ZJGoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUM_CLASSES, return_dict=True)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "XJwTuifiMszz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "plt.figure(figsize=(10, 8))  # 두 개의 별도 그래프를 위한 크기 조절\n",
        "\n",
        "# Training Loss and Accuracy Plot\n",
        "plt.subplot(2, 1, 1)  # 두 개의 서브플롯 중 첫 번째 (2행 1열의 첫 번째)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.title('Training Loss and Accuracy Over Iterations')\n",
        "plt.grid()\n",
        "\n",
        "# Validation Loss and Accuracy Plot\n",
        "plt.subplot(2, 1, 2)  # 두 개의 서브플롯 중 두 번째 (2행 1열의 두 번째)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.title('Validation Loss and Accuracy Over Iterations')\n",
        "plt.grid()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "        batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "        batch_labels = batch[2]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch_inputs).logits\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Append loss to train_losses\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        predicted_labels_train = (outputs > 0.5).float()\n",
        "        correct_train = (predicted_labels_train == batch_labels).all(dim=1).sum()\n",
        "        total_correct_train += correct_train.item()\n",
        "        total_samples_train += batch_labels.size(0)\n",
        "        train_accuracy = total_correct_train / total_samples_train\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Update and display training loss and accuracy plot\n",
        "        plt.figure(figsize=(10, 8))  # 그래프 크기 조절 (학습 손실과 검증 손실을 함께 표시하기 위해 크게 설정)\n",
        "        plt.subplot(2, 1, 1)  # 첫 번째 서브플롯 선택\n",
        "        plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "        plt.plot(range(len(train_accuracies)), train_accuracies, label='Training Accuracy', color='purple')\n",
        "        plt.xlim(0, len(train_losses))\n",
        "        plt.ylim(0, max(max(train_losses), max(train_accuracies)) + 0.1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        # Validation at the end of each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            total_correct_val = 0\n",
        "            total_samples_val = 0\n",
        "\n",
        "            for val_batch_idx, val_batch in enumerate(val_dataloader):\n",
        "                if (val_batch_idx != batch_idx):\n",
        "                        continue\n",
        "                val_batch_inputs = {'input_ids': val_batch[0], 'attention_mask': val_batch[1]}\n",
        "                val_batch_labels = val_batch[2]\n",
        "                val_outputs = model(**val_batch_inputs).logits\n",
        "                val_loss += criterion(val_outputs, val_batch_labels).item()\n",
        "\n",
        "                val_predicted_labels = (val_outputs > 0.5).float()\n",
        "                val_correct = (val_predicted_labels == val_batch_labels).all(dim=1).sum()\n",
        "                total_correct_val += val_correct.item()\n",
        "                total_samples_val += val_batch_labels.size(0)\n",
        "                val_accuracy = total_correct_val / total_samples_val\n",
        "                val_accuracies.append(val_accuracy)\n",
        "                # val_losses.append(val_loss / len(val_dataloader))\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "        # Update and display validation loss and accuracy plot\n",
        "        plt.subplot(2, 1, 2)  # 두 번째 서브플롯 선택\n",
        "        plt.plot(range(len(val_losses)), val_losses, label='Validation Loss', color='red')\n",
        "        plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy', color='green')\n",
        "        plt.xlim(0, len(val_losses))\n",
        "        plt.ylim(0, max(max(val_losses), max(val_accuracies)) + 0.1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(plt.gcf())\n",
        "        time.sleep(0.001)  # 잠시 대기\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "lbWwkQRrWDo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss만 나와서.. accuracy가 표시되도록 새로 만들기로 함\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "plt.figure(figsize=(10, 8))  # 두 개의 별도 그래프를 위한 크기 조절\n",
        "\n",
        "# Training Loss Plot\n",
        "plt.subplot(2, 1, 1)  # 두 개의 서브플롯 중 첫 번째 (2행 1열의 첫 번째)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Iterations')\n",
        "plt.grid()\n",
        "\n",
        "# Validation Loss Plot\n",
        "plt.subplot(2, 1, 2)  # 두 개의 서브플롯 중 두 번째 (2행 1열의 두 번째)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation Loss Over Iterations')\n",
        "plt.grid()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "        batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "        batch_labels = batch[2]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch_inputs).logits\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Append loss to train_losses\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        predicted_labels_train = (outputs > 0.5).float()\n",
        "        correct_train = (predicted_labels_train == batch_labels).all(dim=1).sum()\n",
        "        total_correct_train += correct_train.item()\n",
        "        total_samples_train += batch_labels.size(0)\n",
        "        train_accuracy = total_correct_train / total_samples_train\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Update and display training loss plot\n",
        "        plt.figure(figsize=(10, 8))  # 그래프 크기 조절 (학습 손실과 검증 손실을 함께 표시하기 위해 크게 설정)\n",
        "        plt.subplot(2, 1, 1)  # 첫 번째 서브플롯 선택\n",
        "        plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "        plt.xlim(0, len(train_losses))\n",
        "        plt.ylim(0, max(train_losses) + 0.1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        # Validation at the end of each epoch\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        total_correct_val = 0\n",
        "        total_samples_val = 0\n",
        "\n",
        "        for val_batch_idx, val_batch in enumerate(val_dataloader):\n",
        "            if (val_batch_idx != batch_idx):\n",
        "                    continue\n",
        "            val_batch_inputs = {'input_ids': val_batch[0], 'attention_mask': val_batch[1]}\n",
        "            val_batch_labels = val_batch[2]\n",
        "            val_outputs = model(**val_batch_inputs).logits\n",
        "            val_loss += criterion(val_outputs, val_batch_labels).item()\n",
        "\n",
        "            val_predicted_labels = (val_outputs > 0.5).float()\n",
        "            val_correct = (val_predicted_labels == val_batch_labels).all(dim=1).sum()\n",
        "            total_correct_val += val_correct.item()\n",
        "            total_samples_val += val_batch_labels.size(0)\n",
        "            val_accuracy = total_correct_val / total_samples_val\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss / len(val_dataloader))\n",
        "\n",
        "        # Update and display validation loss plot\n",
        "        plt.subplot(2, 1, 2)  # 두 번째 서브플롯 선택\n",
        "        plt.plot(range(len(val_losses)), val_losses, label='Validation Loss', color='red')\n",
        "        plt.xlim(0, len(val_losses))\n",
        "        plt.ylim(0, max(val_losses) + 0.1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(plt.gcf())\n",
        "        time.sleep(0.001)  # 잠시 대기\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "1WTDoxcAURrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train과 valid loss를 같이 보여주려고 하면서 에러.\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Iterations')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy Over Iterations')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "        batch_labels = batch[2]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch_inputs).logits\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Append loss to train_losses\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        predicted_labels_train = (outputs > 0.5).float()\n",
        "        correct_train = (predicted_labels_train == batch_labels).all(dim=1).sum()\n",
        "        total_correct_train += correct_train.item()\n",
        "        total_samples_train += batch_labels.size(0)\n",
        "        train_accuracy = total_correct_train / total_samples_train\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Update and display training loss and accuracy plot\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "        plt.plot(range(len(val_losses)), val_losses, label='Validation Loss', color='red')\n",
        "        plt.xlim(0, max(len(train_losses), len(val_losses)))\n",
        "        plt.ylim(0, max(max(train_losses), max(val_losses)) + 0.1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(range(len(train_accuracies)), train_accuracies, label='Training Accuracy', color='purple')\n",
        "        plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy', color='green')\n",
        "        plt.xlim(0, max(len(train_accuracies), len(val_accuracies)))\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(plt.gcf())\n",
        "        time.sleep(0.001)  # 잠시 대기\n",
        "\n",
        "        # Validation at mini-batch level (at the end of each training mini-batch)\n",
        "        if (batch_idx + 1) % len(train_dataloader) == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            total_correct_val = 0\n",
        "            total_samples_val = 0\n",
        "\n",
        "            for val_batch_idx, val_batch in enumerate(val_dataloader):\n",
        "                if (val_batch_idx != batch_idx):\n",
        "                    continue\n",
        "                val_batch_inputs = {'input_ids': val_batch[0], 'attention_mask': val_batch[1]}\n",
        "                val_batch_labels = val_batch[2]\n",
        "                val_outputs = model(**val_batch_inputs).logits\n",
        "                val_loss += criterion(val_outputs, val_batch_labels).item()\n",
        "\n",
        "                val_predicted_labels = (val_outputs > 0.5).float()\n",
        "                val_correct = (val_predicted_labels == val_batch_labels).all(dim=1).sum()\n",
        "                total_correct_val += val_correct.item()\n",
        "                total_samples_val += val_batch_labels.size(0)\n",
        "                val_accuracy = total_correct_val / total_samples_val\n",
        "                val_accuracies.append(val_accuracy)\n",
        "                val_losses.append(val_loss / len(val_dataloader))\n"
      ],
      "metadata": {
        "id": "hJa_b-E9Tjpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loss와 validation acc가 잘 나오긴 함\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Iterations')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy Over Iterations')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "        batch_labels = batch[2]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch_inputs).logits\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Append loss to train_losses\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Update and display training loss plot\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "        plt.xlim(0, len(train_losses))\n",
        "        plt.ylim(0, max(train_losses) + 0.1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy', color='green')\n",
        "        plt.xlim(0, len(val_accuracies))\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(plt.gcf())\n",
        "        time.sleep(0.001)  # 잠시 대기\n",
        "\n",
        "        # Validation at mini-batch level\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # for val_batch in val_dataloader:\n",
        "        for val_batch_idx, val_batch in enumerate(val_dataloader):\n",
        "            if (val_batch_idx != batch_idx):\n",
        "                continue\n",
        "            val_batch_inputs = {'input_ids': val_batch[0], 'attention_mask': val_batch[1]}\n",
        "            val_batch_labels = val_batch[2]\n",
        "            val_outputs = model(**val_batch_inputs).logits\n",
        "            val_loss += criterion(val_outputs, val_batch_labels).item()\n",
        "\n",
        "            val_predicted_labels = (val_outputs > 0.5).float()\n",
        "            val_correct = (val_predicted_labels == val_batch_labels).all(dim=1).sum()\n",
        "            total_correct += val_correct.item()\n",
        "            total_samples += val_batch_labels.size(0)\n",
        "\n",
        "            # Append validation loss and accuracy to lists\n",
        "            val_losses.append(val_loss / len(val_dataloader))\n",
        "            val_accuracies.append(total_correct / total_samples)\n"
      ],
      "metadata": {
        "id": "n3dyGhBuSQ9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한번의 트레이닝 미니 배치마다, 전체 valid를 돌고 있어서 문제\n",
        "# train_losses = []\n",
        "# val_losses = []\n",
        "# val_accuracies = []\n",
        "\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title('Training Loss Over Iterations')\n",
        "# plt.grid()\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.title('Validation Accuracy Over Iterations')\n",
        "# plt.ylim(0, 1)\n",
        "# plt.grid()\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(train_dataloader):\n",
        "#         batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "#         batch_labels = batch[2]\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(**batch_inputs).logits\n",
        "#         loss = criterion(outputs, batch_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Append loss to train_losses\n",
        "#         train_losses.append(loss.item())\n",
        "\n",
        "#         # Update and display training loss plot\n",
        "#         plt.figure(figsize=(10, 4))\n",
        "#         plt.subplot(1, 2, 1)\n",
        "#         plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "#         plt.xlim(0, len(train_losses))\n",
        "#         plt.ylim(0, max(train_losses) + 0.1)\n",
        "#         plt.legend()\n",
        "#         plt.grid()\n",
        "\n",
        "#         plt.subplot(1, 2, 2)\n",
        "#         plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy', color='green')\n",
        "#         plt.xlim(0, len(val_accuracies))\n",
        "#         plt.ylim(0, 1)\n",
        "#         plt.legend()\n",
        "#         plt.grid()\n",
        "\n",
        "#         clear_output(wait=True)\n",
        "#         display(plt.gcf())\n",
        "#         time.sleep(0.001)  # 잠시 대기\n",
        "\n",
        "#         # # Validation at mini-batch level\n",
        "#         # model.eval()\n",
        "#         # val_loss = 0.0\n",
        "#         # total_correct = 0\n",
        "#         # total_samples = 0\n",
        "\n",
        "#         # for val_batch in val_dataloader:\n",
        "#         #     val_batch_inputs = {'input_ids': val_batch[0], 'attention_mask': val_batch[1]}\n",
        "#         #     val_batch_labels = val_batch[2]\n",
        "#         #     val_outputs = model(**val_batch_inputs).logits\n",
        "#         #     val_loss += criterion(val_outputs, val_batch_labels).item()\n",
        "\n",
        "#         #     val_predicted_labels = (val_outputs > 0.5).float()\n",
        "#         #     val_correct = (val_predicted_labels == val_batch_labels).all(dim=1).sum()\n",
        "#         #     total_correct += val_correct.item()\n",
        "#         #     total_samples += val_batch_labels.size(0)\n",
        "\n",
        "#         #     # Append validation loss and accuracy to lists\n",
        "#         #     val_losses.append(val_loss / len(val_dataloader))\n",
        "#         #     val_accuracies.append(total_correct / total_samples)\n"
      ],
      "metadata": {
        "id": "4uM1PT1kQr5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_losses = []\n",
        "# val_losses = []\n",
        "# val_accuracies = []\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(train_dataloader):\n",
        "#         batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "#         batch_labels = batch[2]\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(**batch_inputs).logits\n",
        "#         loss = criterion(outputs, batch_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Append loss to train_losses and update loss plot\n",
        "#         train_losses.append(loss.item())\n",
        "#         plt.figure(figsize=(10,4))\n",
        "#         plt.subplot(1,2,1)\n",
        "#         plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "#         plt.xlim(0, len(train_losses)+ 5)\n",
        "#         plt.ylim(0, max(train_losses) + 0.1)\n",
        "\n",
        "#         clear_output(wait=True) # 출력 지우기\n",
        "#         display(plt.gcf())  # 그래프 출력\n",
        "#         time.sleep(0.001) # 잠시 대기\n",
        "\n",
        "#         # Validation at mini-batch level\n",
        "#         model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         total_correct = 0\n",
        "#         total_samples = 0\n",
        "\n",
        "#         for val_batch in val_dataloader:\n",
        "#             val_batch_inputs = {'input_ids': val_batch[0], 'attention_mask': val_batch[1]}\n",
        "#             val_batch_labels = val_batch[2]\n",
        "#             val_outputs = model(**val_batch_inputs).logits\n",
        "#             val_loss += criterion(val_outputs, val_batch_labels).item()\n",
        "\n",
        "#             val_predicted_labels = (val_outputs > 0.5).float()\n",
        "#             val_correct = (val_predicted_labels == val_batch_labels).all(dim=1).sum()\n",
        "#             total_correct += val_correct.item()\n",
        "#             total_samples += val_batch_labels.size(0)\n",
        "\n",
        "#             # Append validation loss and accuracy to lists and update plots\n",
        "#             val_losses.append(val_loss / len(val_dataloader))\n",
        "#             val_accuracies.append(total_correct / total_samples)\n",
        "#             plt.figure(figsize=(10,4))\n",
        "#             plt.subplot(1,2,2)\n",
        "#             plt.plot(range(len(val_losses)), val_losses, label='Validation Loss', color='red')\n",
        "#             plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy', color='green')\n",
        "#             plt.xlim(0, len(val_losses) + 5)\n",
        "#             plt.ylim(0, 1)\n",
        "#             plt.legend()\n",
        "\n",
        "#             clear_output(wait=True)\n",
        "#             display(plt.gcf())\n",
        "#             time.sleep(0.001)\n",
        "\n",
        "#     avg_loss = total_loss / len(train_dataloader)\n",
        "#     print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "hKwo71MpQUZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training loop\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(train_dataloader):\n",
        "#         # print(batch[0], batch[1])\n",
        "#         batch_inputs = {'input_ids':batch[0], 'attention_mask':batch[1]}\n",
        "#         batch_labels = batch[2]\n",
        "#         optimizer.zero_grad()\n",
        "#         # outputs = model(**batch_inputs)[0]\n",
        "#         outputs = model(**batch_inputs).logits\n",
        "#         loss = criterion(outputs, batch_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Append loss to train_losses and update loss plot\n",
        "#         train_losses.append(loss.item())\n",
        "#         plt.figure(figsize=(10,4))\n",
        "#         plt.subplot(1,2,1)\n",
        "#         plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n",
        "#         plt.xlim(0, len(train_losses)+ 5)\n",
        "#         plt.ylim(0, max(train_losses) + 0.1)\n",
        "\n",
        "#         clear_output(wait=True) # 출력 지우기\n",
        "#         display(plt.gcf())  # 그래프 출력\n",
        "#         time.sleep(0.001) # 잠시 대기\n",
        "\n",
        "#     avg_loss = total_loss / len(train_dataloader)\n",
        "#     print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         total_correct = 0\n",
        "#         total_samples = 0\n",
        "#         for batch in val_dataloader:\n",
        "#             batch_inputs = {'input_ids':batch[0], 'attention_mask':batch[1]}\n",
        "#             batch_labels = batch[2]\n",
        "#             outputs = model(**batch_inputs)[0]\n",
        "#             predicted_labels = (outputs > 0.5).float()\n",
        "#             correct = (predicted_labels == batch_labels).all(dim=1).sum()\n",
        "#             total_correct += correct.item()\n",
        "#             total_samples += batch_labels.size(0)\n",
        "\n",
        "#         accuracy = total_correct / total_samples\n",
        "#         print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "#         # Append accuracy to val_accuracies and update accuracy plot\n",
        "#         val_accuracies.append(accuracy)\n",
        "#         plt.figure(figsize=(10,4))\n",
        "#         plt.subplot(1,2,2)\n",
        "#         plt.plot(range(epoch+1), val_accuracies, label='Validation Accuracy', color='green')\n",
        "#         plt.xlim(0, epoch+5)\n",
        "#         plt.ylim(0, 1)\n",
        "#         plt.legend()\n",
        "#         clear_output(wait=True)\n",
        "#         display(plt.gcf())\n",
        "#         time.sleep(0.001)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bdcBekKWJxFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Loss and Accuracy\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CW8_jzWZMpZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}